#!/usr/bin/env python3
"""
End-to-end workflow tests for the complete anime/manga notification system
"""

import pytest
import sqlite3
import asyncio
import json
import tempfile
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime, date, timedelta
import time


class TestCompleteSystemWorkflow:
    """Test complete system workflow from data collection to notification."""
    
    @pytest.mark.e2e
    @pytest.mark.slow
    async def test_full_anime_collection_and_notification_workflow(self, 
                                                                  temp_db, 
                                                                  test_config,
                                                                  mock_anilist_response,
                                                                  mock_gmail_service,
                                                                  mock_calendar_service):
        """Test complete workflow: AniList collection -> Database storage -> Notifications -> Calendar events."""
        
        # Phase 1: Mock AniList API data collection
        with patch('aiohttp.ClientSession') as mock_session_class:
            mock_session = AsyncMock()
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json.return_value = mock_anilist_response
            mock_session.post.return_value.__aenter__.return_value = mock_response
            mock_session_class.return_value.__aenter__.return_value = mock_session
            
            # Simulate AniList data collection
            collected_anime = mock_anilist_response["data"]["Page"]["media"]
            
            # Verify data was collected
            assert len(collected_anime) > 0
            anime_data = collected_anime[0]
            assert "title" in anime_data
            assert "nextAiringEpisode" in anime_data
        
        # Phase 2: Database storage
        conn = sqlite3.connect(temp_db)
        cursor = conn.cursor()
        
        # Store collected anime in database
        anime_title = anime_data["title"]["romaji"]
        
        cursor.execute("""
            INSERT INTO works (title, title_kana, title_en, type, official_url)
            VALUES (?, ?, ?, ?, ?)
        """, (anime_title, "", anime_data["title"].get("english", ""), "anime", anime_data.get("siteUrl")))
        
        work_id = cursor.lastrowid
        
        # Store upcoming episode
        next_episode = anime_data.get("nextAiringEpisode")
        if next_episode:
            airing_date = datetime.fromtimestamp(next_episode["airingAt"]).date()
            
            cursor.execute("""
                INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (work_id, "episode", str(next_episode["episode"]), "dアニメストア", 
                  airing_date.isoformat(), "anilist", anime_data.get("siteUrl"), 0))
            
            release_id = cursor.lastrowid
        
        conn.commit()
        
        # Phase 3: Query unnotified releases
        cursor.execute("""
            SELECT r.*, w.title, w.title_kana, w.title_en, w.type, w.official_url
            FROM releases r
            JOIN works w ON r.work_id = w.id
            WHERE r.notified = 0
            ORDER BY r.release_date ASC
        """)
        
        unnotified_releases = cursor.fetchall()
        assert len(unnotified_releases) == 1
        
        # Phase 4: Generate and send email notifications
        mock_gmail_service.users().messages().send.return_value.execute.return_value = {
            'id': 'sent_email_123',
            'threadId': 'thread_456'
        }
        
        for release_row in unnotified_releases:
            # Generate email content
            email_subject = f"新エピソード配信: {release_row[11]}"  # title from join
            email_body = f"""
            作品: {release_row[11]}
            エピソード: 第{release_row[3]}話
            配信日: {release_row[6]}
            プラットフォーム: {release_row[5]}
            """
            
            # Send email notification
            email_result = mock_gmail_service.users().messages().send(
                userId='me',
                body={'raw': 'base64_encoded_email'}
            ).execute()
            
            assert email_result['id'] == 'sent_email_123'
        
        # Phase 5: Create calendar events
        mock_calendar_service.events().insert.return_value.execute.return_value = {
            'id': 'calendar_event_789',
            'htmlLink': 'https://calendar.google.com/event?eid=calendar_event_789',
            'summary': f'{anime_title} - 第{next_episode["episode"]}話',
            'status': 'confirmed'
        }
        
        for release_row in unnotified_releases:
            # Create calendar event
            event_data = {
                'summary': f'{release_row[11]} - 第{release_row[3]}話',
                'description': f'プラットフォーム: {release_row[5]}',
                'start': {'date': release_row[6], 'timeZone': 'Asia/Tokyo'},
                'end': {'date': release_row[6], 'timeZone': 'Asia/Tokyo'},
                'colorId': '3'
            }
            
            calendar_result = mock_calendar_service.events().insert(
                calendarId='primary',
                body=event_data
            ).execute()
            
            assert calendar_result['id'] == 'calendar_event_789'
        
        # Phase 6: Mark releases as notified
        for release_row in unnotified_releases:
            cursor.execute(
                "UPDATE releases SET notified = 1 WHERE id = ?",
                (release_row[0],)  # release ID
            )
        
        conn.commit()
        
        # Phase 7: Verify workflow completion
        cursor.execute("SELECT COUNT(*) FROM releases WHERE notified = 0")
        remaining_unnotified = cursor.fetchone()[0]
        
        assert remaining_unnotified == 0
        
        # Verify all services were called
        mock_gmail_service.users().messages().send.assert_called()
        mock_calendar_service.events().insert.assert_called()
        
        conn.close()\n    \n    @pytest.mark.e2e\n    @pytest.mark.slow\n    def test_manga_rss_collection_workflow(self, \n                                         temp_db,\n                                         mock_rss_feed_data,\n                                         mock_gmail_service,\n                                         mock_calendar_service):\n        \"\"\"Test complete manga RSS collection workflow.\"\"\"\n        \n        # Phase 1: Mock RSS feed collection\n        with patch('requests.get') as mock_get:\n            mock_response = Mock()\n            mock_response.content = mock_rss_feed_data.encode('utf-8')\n            mock_response.raise_for_status.return_value = None\n            mock_get.return_value = mock_response\n            \n            # Simulate RSS parsing\n            import feedparser\n            feed = feedparser.parse(mock_rss_feed_data)\n            \n            assert len(feed.entries) == 2\n            \n            # Phase 2: Process and store manga data\n            conn = sqlite3.connect(temp_db)\n            cursor = conn.cursor()\n            \n            for entry in feed.entries:\n                # Extract manga info from RSS entry\n                title = entry.title\n                link = entry.link\n                description = entry.description\n                pub_date = entry.published\n                \n                # Parse volume/chapter info from title\n                import re\n                match = re.search(r'第(\\d+)巻', title)\n                volume_number = match.group(1) if match else \"1\"\n                \n                # Extract base title (remove volume info)\n                base_title = re.sub(r'\\s第\\d+巻.*', '', title)\n                \n                # Store work\n                cursor.execute(\"\"\"\n                    INSERT OR IGNORE INTO works (title, type, official_url)\n                    VALUES (?, ?, ?)\n                \"\"\", (base_title, \"manga\", link))\n                \n                # Get work ID\n                cursor.execute(\"SELECT id FROM works WHERE title = ? AND type = ?\", (base_title, \"manga\"))\n                work_id = cursor.fetchone()[0]\n                \n                # Store release\n                cursor.execute(\"\"\"\n                    INSERT OR IGNORE INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\", (work_id, \"volume\", volume_number, \"BookWalker\", \"2024-01-20\", \"rss\", link, 0))\n            \n            conn.commit()\n            \n            # Phase 3: Verify data storage\n            cursor.execute(\"SELECT COUNT(*) FROM works WHERE type = 'manga'\")\n            manga_count = cursor.fetchone()[0]\n            assert manga_count >= 1\n            \n            cursor.execute(\"SELECT COUNT(*) FROM releases WHERE notified = 0\")\n            unnotified_count = cursor.fetchone()[0]\n            assert unnotified_count >= 1\n            \n            # Phase 4: Send notifications (similar to anime workflow)\n            # ... (notification logic would be similar)\n            \n            conn.close()\n    \n    @pytest.mark.e2e\n    def test_error_recovery_workflow(self, temp_db, test_config):\n        \"\"\"Test system behavior during various error conditions.\"\"\"\n        \n        # Test database connection failure recovery\n        with patch('sqlite3.connect') as mock_connect:\n            mock_connect.side_effect = [sqlite3.Error(\"Database locked\"), sqlite3.connect(temp_db)]\n            \n            # First attempt should fail, second should succeed\n            try:\n                conn = mock_connect(temp_db)\n                assert False, \"Should have raised exception on first attempt\"\n            except sqlite3.Error:\n                pass\n            \n            # Retry should succeed\n            conn = mock_connect(temp_db)\n            assert conn is not None\n        \n        # Test API failure recovery\n        with patch('aiohttp.ClientSession') as mock_session_class:\n            mock_session = AsyncMock()\n            \n            # First call fails, second succeeds\n            mock_responses = [\n                AsyncMock(status=500),  # Server error\n                AsyncMock(status=200, json=AsyncMock(return_value={\"data\": {\"Page\": {\"media\": []}}}))\n            ]\n            \n            mock_session.post.return_value.__aenter__.side_effect = mock_responses\n            mock_session_class.return_value.__aenter__.return_value = mock_session\n            \n            # Test retry logic would be implemented here\n        \n        # Test notification failure handling\n        mock_gmail_service = Mock()\n        mock_gmail_service.users().messages().send.return_value.execute.side_effect = [\n            Exception(\"Network error\"),\n            {'id': 'retry_success'}\n        ]\n        \n        # Retry logic test\n        max_retries = 2\n        for attempt in range(max_retries):\n            try:\n                result = mock_gmail_service.users().messages().send().execute()\n                if result:\n                    assert result['id'] == 'retry_success'\n                    break\n            except Exception:\n                if attempt < max_retries - 1:\n                    continue\n                else:\n                    raise\n    \n    @pytest.mark.e2e\n    @pytest.mark.performance\n    def test_high_volume_processing_workflow(self, temp_db, performance_test_config):\n        \"\"\"Test system performance with high volume data processing.\"\"\"\n        \n        start_time = time.time()\n        \n        # Generate large dataset\n        conn = sqlite3.connect(temp_db)\n        cursor = conn.cursor()\n        \n        # Insert many works\n        works_data = []\n        for i in range(100):\n            works_data.append((\n                f\"テストアニメ{i}\",\n                f\"てすとあにめ{i}\",\n                f\"Test Anime {i}\",\n                \"anime\",\n                f\"https://example.com/{i}\"\n            ))\n        \n        cursor.executemany(\"\"\"\n            INSERT INTO works (title, title_kana, title_en, type, official_url)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", works_data)\n        \n        # Insert many releases\n        releases_data = []\n        for i in range(100):\n            releases_data.append((\n                i + 1,  # work_id\n                \"episode\",\n                str(i + 1),\n                \"dアニメストア\",\n                \"2024-01-15\",\n                \"anilist\",\n                f\"https://example.com/{i}\",\n                0\n            ))\n        \n        cursor.executemany(\"\"\"\n            INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\", releases_data)\n        \n        conn.commit()\n        \n        # Test bulk notification processing\n        cursor.execute(\"\"\"\n            SELECT r.*, w.title, w.title_kana, w.title_en, w.type\n            FROM releases r\n            JOIN works w ON r.work_id = w.id\n            WHERE r.notified = 0\n        \"\"\")\n        \n        unnotified_releases = cursor.fetchall()\n        assert len(unnotified_releases) == 100\n        \n        # Simulate bulk notification processing\n        notification_count = 0\n        for release in unnotified_releases:\n            # Simulate email/calendar processing\n            notification_count += 1\n        \n        # Mark all as notified\n        cursor.execute(\"UPDATE releases SET notified = 1 WHERE notified = 0\")\n        conn.commit()\n        \n        end_time = time.time()\n        processing_time = end_time - start_time\n        \n        # Verify performance\n        max_processing_time = performance_test_config.get('max_response_time', 5.0) * 20  # Allow more time for bulk\n        assert processing_time < max_processing_time, f\"Processing took {processing_time:.2f}s, limit is {max_processing_time}s\"\n        assert notification_count == 100\n        \n        # Verify all releases are processed\n        cursor.execute(\"SELECT COUNT(*) FROM releases WHERE notified = 0\")\n        remaining = cursor.fetchone()[0]\n        assert remaining == 0\n        \n        conn.close()\n    \n    @pytest.mark.e2e\n    def test_multi_source_data_integration_workflow(self, \n                                                   temp_db,\n                                                   mock_anilist_response,\n                                                   mock_rss_feed_data):\n        \"\"\"Test integration of data from multiple sources (AniList + RSS).\"\"\"\n        \n        conn = sqlite3.connect(temp_db)\n        cursor = conn.cursor()\n        \n        # Phase 1: Process AniList data\n        anime_data = mock_anilist_response[\"data\"][\"Page\"][\"media\"][0]\n        anime_title = anime_data[\"title\"][\"romaji\"]\n        \n        cursor.execute(\"\"\"\n            INSERT INTO works (title, title_kana, title_en, type, official_url)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (anime_title, \"\", anime_data[\"title\"].get(\"english\", \"\"), \"anime\", anime_data.get(\"siteUrl\")))\n        \n        anime_work_id = cursor.lastrowid\n        \n        # Phase 2: Process RSS data\n        with patch('requests.get') as mock_get:\n            mock_response = Mock()\n            mock_response.content = mock_rss_feed_data.encode('utf-8')\n            mock_response.raise_for_status.return_value = None\n            mock_get.return_value = mock_response\n            \n            import feedparser\n            feed = feedparser.parse(mock_rss_feed_data)\n            \n            for entry in feed.entries:\n                title = entry.title\n                # Extract base title\n                import re\n                base_title = re.sub(r'\\s第\\d+巻.*', '', title)\n                \n                cursor.execute(\"\"\"\n                    INSERT INTO works (title, type, official_url)\n                    VALUES (?, ?, ?)\n                \"\"\", (base_title, \"manga\", entry.link))\n                \n                manga_work_id = cursor.lastrowid\n        \n        conn.commit()\n        \n        # Phase 3: Verify multi-source integration\n        cursor.execute(\"SELECT type, COUNT(*) FROM works GROUP BY type\")\n        work_counts = dict(cursor.fetchall())\n        \n        assert \"anime\" in work_counts\n        assert \"manga\" in work_counts\n        assert work_counts[\"anime\"] >= 1\n        assert work_counts[\"manga\"] >= 1\n        \n        # Phase 4: Test unified notification processing\n        cursor.execute(\"\"\"\n            SELECT w.type, COUNT(*) as count\n            FROM works w\n            GROUP BY w.type\n        \"\"\")\n        \n        type_counts = dict(cursor.fetchall())\n        total_works = sum(type_counts.values())\n        \n        assert total_works >= 2  # At least one anime and one manga\n        \n        conn.close()\n    \n    @pytest.mark.e2e\n    def test_configuration_driven_workflow(self, temp_db):\n        \"\"\"Test that workflow behavior changes based on configuration.\"\"\"\n        \n        # Test with email enabled, calendar disabled\n        config_email_only = {\n            \"notification\": {\n                \"email\": {\"enabled\": True},\n                \"calendar\": {\"enabled\": False}\n            },\n            \"filtering\": {\n                \"ng_keywords\": [\"エロ\", \"R18\"]\n            }\n        }\n        \n        # Test with calendar enabled, email disabled\n        config_calendar_only = {\n            \"notification\": {\n                \"email\": {\"enabled\": False},\n                \"calendar\": {\"enabled\": True}\n            },\n            \"filtering\": {\n                \"ng_keywords\": []\n            }\n        }\n        \n        # Test workflow behavior changes based on config\n        for config in [config_email_only, config_calendar_only]:\n            email_enabled = config[\"notification\"][\"email\"][\"enabled\"]\n            calendar_enabled = config[\"notification\"][\"calendar\"][\"enabled\"]\n            ng_keywords = config[\"filtering\"][\"ng_keywords\"]\n            \n            # Verify configuration affects workflow decisions\n            assert email_enabled != calendar_enabled  # One should be enabled, other disabled\n            \n            if email_enabled:\n                # Email workflow would be executed\n                assert config == config_email_only\n            \n            if calendar_enabled:\n                # Calendar workflow would be executed\n                assert config == config_calendar_only\n    \n    @pytest.mark.e2e\n    def test_data_consistency_and_deduplication_workflow(self, temp_db):\n        \"\"\"Test data consistency and deduplication across the workflow.\"\"\"\n        \n        conn = sqlite3.connect(temp_db)\n        cursor = conn.cursor()\n        \n        # Phase 1: Insert duplicate work data\n        work_data = (\"ワンピース\", \"わんぴーす\", \"One Piece\", \"anime\", \"https://one-piece.com\")\n        \n        # Try to insert same work twice\n        cursor.execute(\"\"\"\n            INSERT INTO works (title, title_kana, title_en, type, official_url)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", work_data)\n        \n        work_id_1 = cursor.lastrowid\n        \n        # Second insertion should be handled by application logic (get_or_create pattern)\n        cursor.execute(\"SELECT id FROM works WHERE title = ? AND type = ?\", (work_data[0], work_data[3]))\n        existing_work = cursor.fetchone()\n        \n        if existing_work:\n            work_id_2 = existing_work[0]\n        else:\n            cursor.execute(\"\"\"\n                INSERT INTO works (title, title_kana, title_en, type, official_url)\n                VALUES (?, ?, ?, ?, ?)\n            \"\"\", work_data)\n            work_id_2 = cursor.lastrowid\n        \n        # Should be the same work ID (deduplication)\n        assert work_id_1 == work_id_2\n        \n        # Phase 2: Test release deduplication\n        release_data = (work_id_1, \"episode\", \"1050\", \"dアニメストア\", \"2024-01-15\", \"anilist\", \"https://example.com\", 0)\n        \n        cursor.execute(\"\"\"\n            INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\", release_data)\n        \n        release_id_1 = cursor.lastrowid\n        \n        # Try to insert duplicate release (should be blocked by UNIQUE constraint)\n        try:\n            cursor.execute(\"\"\"\n                INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", release_data)\n            conn.commit()\n            assert False, \"Should have failed due to UNIQUE constraint\"\n        except sqlite3.IntegrityError:\n            # Expected behavior\n            pass\n        \n        # Phase 3: Verify data consistency\n        cursor.execute(\"SELECT COUNT(*) FROM works WHERE title = ?\", (work_data[0],))\n        work_count = cursor.fetchone()[0]\n        assert work_count == 1  # Only one work should exist\n        \n        cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM releases \n            WHERE work_id = ? AND release_type = ? AND number = ? AND platform = ?\n        \"\"\", (work_id_1, \"episode\", \"1050\", \"dアニメストア\"))\n        release_count = cursor.fetchone()[0]\n        assert release_count == 1  # Only one release should exist\n        \n        conn.close()\n\n\nclass TestSystemIntegrationScenarios:\n    \"\"\"Test various real-world integration scenarios.\"\"\"\n    \n    @pytest.mark.e2e\n    @pytest.mark.integration\n    def test_weekly_anime_series_tracking_scenario(self, temp_db):\n        \"\"\"Test tracking weekly anime series over multiple episodes.\"\"\"\n        \n        conn = sqlite3.connect(temp_db)\n        cursor = conn.cursor()\n        \n        # Setup: Create ongoing anime series\n        cursor.execute(\"\"\"\n            INSERT INTO works (title, title_kana, title_en, type, official_url)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (\"呪術廻戦\", \"じゅじつかいせん\", \"Jujutsu Kaisen\", \"anime\", \"https://jujutsukaisen.jp\"))\n        \n        work_id = cursor.lastrowid\n        \n        # Simulate weekly episode releases over a season (12 episodes)\n        base_date = date(2024, 1, 7)  # Start on Sunday\n        \n        for episode_num in range(1, 13):\n            release_date = base_date + timedelta(weeks=episode_num-1)\n            \n            cursor.execute(\"\"\"\n                INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (work_id, \"episode\", str(episode_num), \"Crunchyroll\", release_date.isoformat(), \"anilist\", \n                   f\"https://anilist.co/anime/呪術廻戦/{episode_num}\", 0))\n        \n        conn.commit()\n        \n        # Test: Process notifications in chronological order\n        cursor.execute(\"\"\"\n            SELECT r.*, w.title\n            FROM releases r\n            JOIN works w ON r.work_id = w.id\n            WHERE r.notified = 0\n            ORDER BY r.release_date ASC\n        \"\"\")\n        \n        unnotified_releases = cursor.fetchall()\n        assert len(unnotified_releases) == 12\n        \n        # Simulate processing episodes as they air\n        processed_episodes = []\n        for release in unnotified_releases[:3]:  # Process first 3 episodes\n            episode_num = release[3]  # number column\n            processed_episodes.append(int(episode_num))\n            \n            # Mark as notified\n            cursor.execute(\"UPDATE releases SET notified = 1 WHERE id = ?\", (release[0],))\n        \n        conn.commit()\n        \n        # Verify: Episodes processed in order\n        assert processed_episodes == [1, 2, 3]\n        \n        # Verify: Remaining episodes still unnotified\n        cursor.execute(\"SELECT COUNT(*) FROM releases WHERE notified = 0\")\n        remaining_count = cursor.fetchone()[0]\n        assert remaining_count == 9\n        \n        conn.close()\n    \n    @pytest.mark.e2e\n    @pytest.mark.integration\n    def test_manga_series_completion_scenario(self, temp_db):\n        \"\"\"Test handling manga series completion and final volume notifications.\"\"\"\n        \n        conn = sqlite3.connect(temp_db)\n        cursor = conn.cursor()\n        \n        # Setup: Create manga series nearing completion\n        cursor.execute(\"\"\"\n            INSERT INTO works (title, title_kana, title_en, type, official_url)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (\"進撃の巨人\", \"しんげきのきょじん\", \"Attack on Titan\", \"manga\", \"https://shingeki.tv\"))\n        \n        work_id = cursor.lastrowid\n        \n        # Add final few volumes\n        final_volumes = [32, 33, 34]  # Last 3 volumes\n        \n        for vol_num in final_volumes:\n            is_final = (vol_num == 34)\n            description = \"完結\" if is_final else f\"第{vol_num}巻\"\n            \n            cursor.execute(\"\"\"\n                INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (work_id, \"volume\", str(vol_num), \"BookWalker\", \"2024-01-15\", \"rss\", \n                   f\"https://bookwalker.jp/shingeki/{vol_num}\", 0))\n        \n        conn.commit()\n        \n        # Test: Special handling for final volume\n        cursor.execute(\"\"\"\n            SELECT r.*, w.title\n            FROM releases r\n            JOIN works w ON r.work_id = w.id\n            WHERE r.notified = 0\n            ORDER BY CAST(r.number AS INTEGER) ASC\n        \"\"\")\n        \n        releases = cursor.fetchall()\n        \n        # Process each volume with special handling for final\n        for release in releases:\n            volume_num = int(release[3])\n            is_final_volume = (volume_num == 34)\n            \n            if is_final_volume:\n                # Special notification for series completion\n                notification_type = \"series_completion\"\n                priority = \"high\"\n            else:\n                notification_type = \"regular\"\n                priority = \"normal\"\n            \n            # Verify special handling for final volume\n            if is_final_volume:\n                assert notification_type == \"series_completion\"\n                assert priority == \"high\"\n        \n        conn.close()\n    \n    @pytest.mark.e2e\n    @pytest.mark.integration\n    def test_seasonal_anime_batch_processing_scenario(self, temp_db):\n        \"\"\"Test processing entire anime season announcements.\"\"\"\n        \n        conn = sqlite3.connect(temp_db)\n        cursor = conn.cursor()\n        \n        # Setup: Winter 2024 anime season\n        winter_2024_anime = [\n            (\"薬屋のひとりごと\", \"The Apothecary Diaries\"),\n            (\"ダンジョン飯\", \"Delicious in Dungeon\"),\n            (\"俺だけレベルアップな件\", \"Solo Leveling\"),\n            (\"青の祓魔師 島根啓明結社篇\", \"Blue Exorcist\"),\n            (\"魔都精兵のスレイブ\", \"Chained Soldier\")\n        ]\n        \n        work_ids = []\n        for jp_title, en_title in winter_2024_anime:\n            cursor.execute(\"\"\"\n                INSERT INTO works (title, title_kana, title_en, type, official_url)\n                VALUES (?, ?, ?, ?, ?)\n            \"\"\", (jp_title, \"\", en_title, \"anime\", f\"https://example.com/{jp_title}\"))\n            \n            work_ids.append(cursor.lastrowid)\n        \n        # Add first episodes for all anime (season premiere)\n        season_start_date = date(2024, 1, 7)\n        \n        for i, work_id in enumerate(work_ids):\n            # Stagger premiere dates (different days of the week)\n            premiere_date = season_start_date + timedelta(days=i)\n            \n            cursor.execute(\"\"\"\n                INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (work_id, \"episode\", \"1\", \"各種配信サイト\", premiere_date.isoformat(), \"anilist\", \n                   f\"https://anilist.co/anime/{work_id}\", 0))\n        \n        conn.commit()\n        \n        # Test: Batch process season premieres\n        cursor.execute(\"\"\"\n            SELECT r.*, w.title, w.title_en\n            FROM releases r\n            JOIN works w ON r.work_id = w.id\n            WHERE r.notified = 0 AND r.number = '1'\n            ORDER BY r.release_date ASC\n        \"\"\")\n        \n        premieres = cursor.fetchall()\n        assert len(premieres) == 5\n        \n        # Simulate seasonal announcement email\n        premiere_titles = [release[11] for release in premieres]  # title column\n        \n        season_announcement = {\n            \"type\": \"seasonal_announcement\",\n            \"season\": \"Winter 2024\",\n            \"anime_count\": len(premiere_titles),\n            \"titles\": premiere_titles\n        }\n        \n        # Verify all winter 2024 titles are included\n        for jp_title, _ in winter_2024_anime:\n            assert jp_title in season_announcement[\"titles\"]\n        \n        conn.close()\n    \n    @pytest.mark.e2e\n    @pytest.mark.integration\n    def test_multi_platform_availability_scenario(self, temp_db):\n        \"\"\"Test handling anime/manga available on multiple platforms.\"\"\"\n        \n        conn = sqlite3.connect(temp_db)\n        cursor = conn.cursor()\n        \n        # Setup: Popular anime available on multiple platforms\n        cursor.execute(\"\"\"\n            INSERT INTO works (title, title_kana, title_en, type, official_url)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (\"鬼滅の刃\", \"きめつのやいば\", \"Demon Slayer\", \"anime\", \"https://kimetsu.com\"))\n        \n        work_id = cursor.lastrowid\n        \n        # Add same episode across multiple platforms\n        platforms = [\n            (\"Netflix\", \"2024-01-15\"),\n            (\"Crunchyroll\", \"2024-01-15\"),\n            (\"dアニメストア\", \"2024-01-15\"),\n            (\"Amazon Prime Video\", \"2024-01-16\")  # Slightly delayed\n        ]\n        \n        for platform, release_date in platforms:\n            cursor.execute(\"\"\"\n                INSERT INTO releases (work_id, release_type, number, platform, release_date, source, source_url, notified)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (work_id, \"episode\", \"1\", platform, release_date, \"anilist\", \n                   f\"https://{platform.lower().replace(' ', '')}.com/kimetsu/ep1\", 0))\n        \n        conn.commit()\n        \n        # Test: Group notifications by anime, show all platforms\n        cursor.execute(\"\"\"\n            SELECT r.platform, r.release_date, w.title\n            FROM releases r\n            JOIN works w ON r.work_id = w.id\n            WHERE r.work_id = ? AND r.number = '1'\n            ORDER BY r.release_date ASC, r.platform ASC\n        \"\"\", (work_id,))\n        \n        platform_releases = cursor.fetchall()\n        assert len(platform_releases) == 4\n        \n        # Group by release date for notification optimization\n        from collections import defaultdict\n        releases_by_date = defaultdict(list)\n        \n        for release in platform_releases:\n            platform = release[0]\n            release_date = release[1]\n            releases_by_date[release_date].append(platform)\n        \n        # Verify grouping\n        assert len(releases_by_date[\"2024-01-15\"]) == 3  # Netflix, Crunchyroll, dアニメストア\n        assert len(releases_by_date[\"2024-01-16\"]) == 1  # Amazon Prime Video\n        \n        # Test: Single notification with multiple platform info\n        earliest_date = min(releases_by_date.keys())\n        platforms_on_earliest = releases_by_date[earliest_date]\n        \n        notification_content = {\n            \"title\": \"鬼滅の刃\",\n            \"episode\": \"1\",\n            \"primary_date\": earliest_date,\n            \"platforms\": platforms_on_earliest,\n            \"additional_dates\": {date: platforms for date, platforms in releases_by_date.items() if date != earliest_date}\n        }\n        \n        assert len(notification_content[\"platforms\"]) == 3\n        assert \"Netflix\" in notification_content[\"platforms\"]\n        \n        conn.close()"