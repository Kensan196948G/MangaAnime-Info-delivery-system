#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿åé›†æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä½œæˆæ—¥: 2025-12-06
ç›®çš„: ãƒ‡ãƒ¼ã‚¿åé›†å¾Œã®å“è³ªãƒã‚§ãƒƒã‚¯ã¨çµ±è¨ˆæƒ…å ±å‡ºåŠ›
"""

import sqlite3
import json
import os
import sys
from datetime import datetime
from collections import defaultdict

PROJECT_ROOT = "/mnt/Linux-ExHDD/MangaAnime-Info-delivery-system"
DB_PATH = os.path.join(PROJECT_ROOT, "db.sqlite3")

def connect_db():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š"""
    if not os.path.exists(DB_PATH):
        logger.info(f"âœ— ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {DB_PATH}")
        sys.exit(1)
    return sqlite3.connect(DB_PATH)

def get_statistics(conn):
    """çµ±è¨ˆæƒ…å ±å–å¾—"""
    cursor = conn.cursor()

    stats = {
        "works": {},
        "releases": {},
        "data_quality": {}
    }

    # Worksçµ±è¨ˆ
    cursor.execute("SELECT COUNT(*) FROM works")
    stats["works"]["total_count"] = cursor.fetchone()[0]

    cursor.execute("SELECT type, COUNT(*) FROM works GROUP BY type")
    stats["works"]["by_type"] = dict(cursor.fetchall())

    cursor.execute("SELECT COUNT(*) FROM works WHERE title_kana IS NOT NULL")
    stats["works"]["with_kana"] = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM works WHERE official_url IS NOT NULL")
    stats["works"]["with_url"] = cursor.fetchone()[0]

    # Releasesçµ±è¨ˆ
    cursor.execute("SELECT COUNT(*) FROM releases")
    stats["releases"]["total_count"] = cursor.fetchone()[0]

    cursor.execute("SELECT release_type, COUNT(*) FROM releases GROUP BY release_type")
    stats["releases"]["by_type"] = dict(cursor.fetchall())

    cursor.execute("SELECT platform, COUNT(*) FROM releases GROUP BY platform")
    stats["releases"]["by_platform"] = dict(cursor.fetchall())

    cursor.execute("SELECT source, COUNT(*) FROM releases GROUP BY source")
    stats["releases"]["by_source"] = dict(cursor.fetchall())

    cursor.execute("SELECT COUNT(*) FROM releases WHERE notified = 1")
    stats["releases"]["notified_count"] = cursor.fetchone()[0]

    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    cursor.execute("SELECT COUNT(*) FROM works WHERE title IS NULL OR title = ''")
    stats["data_quality"]["works_without_title"] = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM releases WHERE release_date IS NULL")
    stats["data_quality"]["releases_without_date"] = cursor.fetchone()[0]

    cursor.execute("""
        SELECT work_id, COUNT(*) as dup_count
        FROM releases
        GROUP BY work_id, release_type, number, platform, release_date
        HAVING COUNT(*) > 1
    """)
    stats["data_quality"]["duplicate_releases"] = len(cursor.fetchall())

    return stats

def get_recent_data(conn, limit=10):
    """æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å–å¾—"""
    cursor = conn.cursor()

    recent = {
        "works": [],
        "releases": []
    }

    cursor.execute("""
        SELECT id, title, type, official_url, created_at
        FROM works
        ORDER BY created_at DESC
        LIMIT ?
    """, (limit,))
    recent["works"] = cursor.fetchall()

    cursor.execute("""
        SELECT r.id, w.title, r.release_type, r.number, r.platform, r.release_date, r.created_at
        FROM releases r
        JOIN works w ON r.work_id = w.id
        ORDER BY r.created_at DESC
        LIMIT ?
    """, (limit,))
    recent["releases"] = cursor.fetchall()

    return recent

def print_report(stats, recent):
    """ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›"""
    logger.info("\n" + "="*60)
    logger.info("ãƒ‡ãƒ¼ã‚¿åé›†æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
    logger.info("="*60)
    logger.info(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")

    # Worksçµ±è¨ˆ
    logger.info("ğŸ“Š Worksçµ±è¨ˆ")
    logger.info("-" * 60)
    logger.info(f"  ç·ä½œå“æ•°: {stats['works']['total_count']}")
    logger.info(f"  ã‚¿ã‚¤ãƒ—åˆ¥:")
    for work_type, count in stats['works']['by_type'].items():
        logger.info(f"    - {work_type}: {count}")
    logger.info(f"  èª­ã¿ä»®åã‚ã‚Š: {stats['works']['with_kana']} ({stats['works']['with_kana']/max(stats['works']['total_count'],1)*100:.1f}%)")
    logger.info(f"  å…¬å¼URLã‚ã‚Š: {stats['works']['with_url']} ({stats['works']['with_url']/max(stats['works']['total_count'],1)*100:.1f}%)")
    logger.info("")

    # Releasesçµ±è¨ˆ
    logger.info("ğŸ“… Releasesçµ±è¨ˆ")
    logger.info("-" * 60)
    logger.info(f"  ç·ãƒªãƒªãƒ¼ã‚¹æ•°: {stats['releases']['total_count']}")
    logger.info(f"  ãƒªãƒªãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—åˆ¥:")
    for release_type, count in stats['releases']['by_type'].items():
        logger.info(f"    - {release_type}: {count}")
    logger.info(f"  ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥:")
    for platform, count in sorted(stats['releases']['by_platform'].items(), key=lambda x: x[1], reverse=True)[:10]:
        logger.info(f"    - {platform}: {count}")
    logger.info(f"  ã‚½ãƒ¼ã‚¹åˆ¥:")
    for source, count in stats['releases']['by_source'].items():
        logger.info(f"    - {source}: {count}")
    logger.info(f"  é€šçŸ¥æ¸ˆã¿: {stats['releases']['notified_count']}")
    logger.info("")

    # ãƒ‡ãƒ¼ã‚¿å“è³ª
    logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯")
    logger.info("-" * 60)
    quality_issues = 0
    if stats['data_quality']['works_without_title'] > 0:
        logger.info(f"  âš  ã‚¿ã‚¤ãƒˆãƒ«ãªã—ã®ä½œå“: {stats['data_quality']['works_without_title']}")
        quality_issues += 1
    if stats['data_quality']['releases_without_date'] > 0:
        logger.info(f"  âš  é…ä¿¡æ—¥ãªã—ã®ãƒªãƒªãƒ¼ã‚¹: {stats['data_quality']['releases_without_date']}")
        quality_issues += 1
    if stats['data_quality']['duplicate_releases'] > 0:
        logger.info(f"  âš  é‡è¤‡ãƒªãƒªãƒ¼ã‚¹: {stats['data_quality']['duplicate_releases']}")
        quality_issues += 1

    if quality_issues == 0:
        logger.info("  âœ“ ãƒ‡ãƒ¼ã‚¿å“è³ªã«å•é¡Œãªã—")
    logger.info("")

    # æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    logger.info("ğŸ†• æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸä½œå“ (ä¸Šä½5ä»¶)")
    logger.info("-" * 60)
    for work in recent['works'][:5]:
        work_id, title, work_type, url, created_at = work
        logger.info(f"  [{work_id}] {title} ({work_type})")
        logger.info(f"      URL: {url if url else 'N/A'}")
        logger.info(f"      è¿½åŠ æ—¥æ™‚: {created_at}")
    logger.info("")

    logger.info("ğŸ†• æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒªãƒªãƒ¼ã‚¹ (ä¸Šä½5ä»¶)")
    logger.info("-" * 60)
    for release in recent['releases'][:5]:
        rel_id, title, rel_type, number, platform, rel_date, created_at = release
        logger.info(f"  [{rel_id}] {title}")
        logger.info(f"      {rel_type} #{number if number else 'N/A'} on {platform}")
        logger.info(f"      é…ä¿¡æ—¥: {rel_date if rel_date else 'N/A'}")
        logger.info(f"      è¿½åŠ æ—¥æ™‚: {created_at}")
    logger.info("")

    logger.info("="*60)
    logger.info("æ¤œè¨¼å®Œäº†")
    logger.info("="*60)

def export_json_report(stats, output_path):
    """JSONå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›"""
    report = {
        "generated_at": datetime.now().isoformat(),
        "statistics": stats
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    logger.info(f"\nğŸ“„ JSON ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {output_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
import logging

logger = logging.getLogger(__name__)

    logger.info("ãƒ‡ãƒ¼ã‚¿åé›†æ¤œè¨¼ã‚’é–‹å§‹ã—ã¾ã™...")

logger = logging.getLogger(__name__)


    conn = connect_db()

    try:
        stats = get_statistics(conn)
        recent = get_recent_data(conn, limit=10)

        print_report(stats, recent)

        # JSONãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        json_path = os.path.join(PROJECT_ROOT, "logs", "data_collection_report.json")
        os.makedirs(os.path.dirname(json_path), exist_ok=True)
        export_json_report(stats, json_path)

    finally:
        conn.close()

if __name__ == "__main__":
    main()
