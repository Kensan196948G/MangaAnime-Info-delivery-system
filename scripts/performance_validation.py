#!/usr/bin/env python3
"""
ã‚¢ãƒ‹ãƒ¡ãƒ»ãƒãƒ³ã‚¬æƒ…å ±é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»é‹ç”¨æ€§æœ€çµ‚æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Performance and Operational Validation Suite
"""

import time
import asyncio
import aiohttp
import psutil
import sqlite3
import json
import logging
import subprocess
import os
import sys
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import requests
from typing import Dict, List, Tuple, Any
import threading
import traceback

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/mnt/Linux-ExHDD/MangaAnime-Info-delivery-system/logs/performance_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PerformanceValidator:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»é‹ç”¨æ€§æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_root = Path("/mnt/Linux-ExHDD/MangaAnime-Info-delivery-system")
        self.db_path = self.project_root / "db.sqlite3"
        self.config_path = self.project_root / "config" / "config.json"
        self.results = {}
        self.start_time = time.time()
        
    def load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    async def measure_api_performance(self) -> Dict[str, Any]:
        """APIãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
        logger.info("ğŸš€ APIãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šé–‹å§‹")
        
        results = {
            "anilist_response_times": [],
            "rss_response_times": [],
            "api_success_rate": 0,
            "concurrent_request_performance": {}
        }
        
        # AniList GraphQL APIå¿œç­”æ™‚é–“æ¸¬å®š
        anilist_query = """
        query {
            Page(page: 1, perPage: 10) {
                media(type: ANIME, status: RELEASING) {
                    id
                    title {
                        romaji
                        english
                        native
                    }
                    startDate {
                        year
                        month
                        day
                    }
                    episodes
                    genres
                    streamingEpisodes {
                        title
                        url
                    }
                }
            }
        }
        """
        
        async with aiohttp.ClientSession() as session:
            # AniList APIæ¸¬å®šï¼ˆ10å›ï¼‰
            for i in range(10):
                start_time = time.time()
                try:
                    async with session.post(
                        'https://graphql.anilist.co',
                        json={'query': anilist_query},
                        headers={'Content-Type': 'application/json'}
                    ) as response:
                        await response.json()
                        response_time = (time.time() - start_time) * 1000
                        results["anilist_response_times"].append(response_time)
                        logger.info(f"AniList API #{i+1}: {response_time:.2f}ms")
                except Exception as e:
                    logger.error(f"AniList API ã‚¨ãƒ©ãƒ¼ #{i+1}: {e}")
                    results["anilist_response_times"].append(999999)
                
                await asyncio.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            
            # RSS ãƒ•ã‚£ãƒ¼ãƒ‰å¿œç­”æ™‚é–“æ¸¬å®š
            rss_urls = [
                "https://anime.dmkt-sp.jp/animestore/CF/rss/",
                "https://bookwalker.jp/rss/",
                "https://pocket.shonenmagazine.com/rss/episode"
            ]
            
            for url in rss_urls:
                start_time = time.time()
                try:
                    async with session.get(url) as response:
                        await response.text()
                        response_time = (time.time() - start_time) * 1000
                        results["rss_response_times"].append(response_time)
                        logger.info(f"RSS {url}: {response_time:.2f}ms")
                except Exception as e:
                    logger.error(f"RSS {url} ã‚¨ãƒ©ãƒ¼: {e}")
                    results["rss_response_times"].append(999999)
        
        # æˆåŠŸç‡è¨ˆç®—
        total_requests = len(results["anilist_response_times"]) + len(results["rss_response_times"])
        failed_requests = sum(1 for t in results["anilist_response_times"] + results["rss_response_times"] if t > 30000)
        results["api_success_rate"] = ((total_requests - failed_requests) / total_requests) * 100
        
        return results
    
    def measure_database_performance(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
        logger.info("ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šé–‹å§‹")
        
        results = {
            "insert_performance": [],
            "select_performance": [],
            "concurrent_operations": {},
            "database_size": 0
        }
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºå–å¾—
            results["database_size"] = os.path.getsize(self.db_path) if os.path.exists(self.db_path) else 0
            
            # å¤§é‡INSERTæ€§èƒ½æ¸¬å®š
            test_data = [
                (f"ãƒ†ã‚¹ãƒˆä½œå“{i}", "anime", f"https://example.com/{i}", datetime.now())
                for i in range(1000)
            ]
            
            start_time = time.time()
            cursor.executemany(
                "INSERT OR IGNORE INTO works (title, type, official_url, created_at) VALUES (?, ?, ?, ?)",
                test_data
            )
            conn.commit()
            insert_time = (time.time() - start_time) * 1000
            results["insert_performance"] = insert_time
            logger.info(f"1000ä»¶INSERT: {insert_time:.2f}ms")
            
            # SELECTæ€§èƒ½æ¸¬å®š
            start_time = time.time()
            cursor.execute("SELECT * FROM works WHERE type = 'anime' LIMIT 100")
            rows = cursor.fetchall()
            select_time = (time.time() - start_time) * 1000
            results["select_performance"] = select_time
            logger.info(f"100ä»¶SELECT: {select_time:.2f}ms")
            
            # åŒæ™‚æ“ä½œæ€§èƒ½
            def concurrent_db_operation(thread_id):
                conn_local = sqlite3.connect(self.db_path)
                cursor_local = conn_local.cursor()
                start = time.time()
                cursor_local.execute("SELECT COUNT(*) FROM works")
                result = cursor_local.fetchone()
                elapsed = (time.time() - start) * 1000
                conn_local.close()
                return thread_id, elapsed
            
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(concurrent_db_operation, i) for i in range(10)]
                concurrent_times = []
                for future in as_completed(futures):
                    thread_id, elapsed = future.result()
                    concurrent_times.append(elapsed)
                
                results["concurrent_operations"] = {
                    "average_time": sum(concurrent_times) / len(concurrent_times),
                    "max_time": max(concurrent_times),
                    "min_time": min(concurrent_times)
                }
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cursor.execute("DELETE FROM works WHERE title LIKE 'ãƒ†ã‚¹ãƒˆä½œå“%'")
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            
        return results
    
    def measure_system_resources(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¸¬å®š"""
        logger.info("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¸¬å®šé–‹å§‹")
        
        results = {
            "cpu_usage": psutil.cpu_percent(interval=1),
            "memory_usage": psutil.virtual_memory()._asdict(),
            "disk_usage": psutil.disk_usage('/')._asdict(),
            "process_count": len(psutil.pids()),
            "network_io": psutil.net_io_counters()._asdict() if psutil.net_io_counters() else {},
        }
        
        # Pythonãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°ç›£è¦–
        try:
            current_process = psutil.Process()
            results["python_process"] = {
                "memory_info": current_process.memory_info()._asdict(),
                "cpu_percent": current_process.cpu_percent(),
                "threads": current_process.num_threads(),
                "open_files": len(current_process.open_files())
            }
        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
            
        return results
    
    def test_scalability(self) -> Dict[str, Any]:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼"""
        logger.info("ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼é–‹å§‹")
        
        results = {
            "large_data_processing": {},
            "concurrent_users": {},
            "memory_efficiency": {}
        }
        
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
        try:
            start_memory = psutil.virtual_memory().used
            start_time = time.time()
            
            # 1000ä»¶ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ»å‡¦ç†
            large_dataset = []
            for i in range(1000):
                large_dataset.append({
                    "id": i,
                    "title": f"ã‚¢ãƒ‹ãƒ¡ä½œå“{i}",
                    "release_date": datetime.now() + timedelta(days=i),
                    "genres": ["ã‚¢ã‚¯ã‚·ãƒ§ãƒ³", "ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼", "ã‚³ãƒ¡ãƒ‡ã‚£"],
                    "description": "ã“ã‚Œã¯å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼èª¬æ˜æ–‡ã§ã™ã€‚" * 10
                })
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            processed_data = []
            for item in large_dataset:
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
                if "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³" in item["genres"]:
                    processed_data.append({
                        "title": item["title"],
                        "processed_at": datetime.now(),
                        "summary": item["description"][:100]
                    })
            
            end_time = time.time()
            end_memory = psutil.virtual_memory().used
            
            results["large_data_processing"] = {
                "processing_time": (end_time - start_time) * 1000,
                "memory_usage": end_memory - start_memory,
                "processed_items": len(processed_data),
                "throughput": len(processed_data) / (end_time - start_time)
            }
            
        except Exception as e:
            logger.error(f"å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def validate_operational_automation(self) -> Dict[str, Any]:
        """é‹ç”¨è‡ªå‹•åŒ–æ©Ÿèƒ½æ¤œè¨¼"""
        logger.info("ğŸ”§ é‹ç”¨è‡ªå‹•åŒ–æ©Ÿèƒ½æ¤œè¨¼é–‹å§‹")
        
        results = {
            "cron_configuration": {},
            "log_management": {},
            "backup_functionality": {},
            "error_recovery": {}
        }
        
        # cronè¨­å®šç¢ºèª
        try:
            cron_check = subprocess.run(['crontab', '-l'], capture_output=True, text=True)
            if cron_check.returncode == 0:
                cron_lines = cron_check.stdout.strip().split('\n')
                anime_cron = [line for line in cron_lines if 'release_notifier.py' in line]
                results["cron_configuration"] = {
                    "configured": len(anime_cron) > 0,
                    "entries": anime_cron,
                    "total_cron_jobs": len(cron_lines)
                }
            else:
                results["cron_configuration"] = {"configured": False, "error": cron_check.stderr}
        except Exception as e:
            logger.error(f"cronè¨­å®šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            results["cron_configuration"] = {"configured": False, "error": str(e)}
        
        # ãƒ­ã‚°ç®¡ç†æ©Ÿèƒ½ç¢ºèª
        log_dir = self.project_root / "logs"
        if log_dir.exists():
            log_files = list(log_dir.glob("*.log"))
            total_log_size = sum(f.stat().st_size for f in log_files)
            results["log_management"] = {
                "log_directory_exists": True,
                "log_files_count": len(log_files),
                "total_log_size": total_log_size,
                "log_files": [f.name for f in log_files]
            }
        else:
            results["log_management"] = {"log_directory_exists": False}
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ç¢ºèª
        backup_script = self.project_root / "scripts" / "backup.sh"
        results["backup_functionality"] = {
            "backup_script_exists": backup_script.exists(),
            "database_exists": self.db_path.exists(),
            "config_backup_ready": self.config_path.exists()
        }
        
        # ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½ç¢ºèª
        error_recovery_module = self.project_root / "modules" / "enhanced_error_recovery.py"
        results["error_recovery"] = {
            "error_recovery_module_exists": error_recovery_module.exists(),
            "auto_retry_configured": True,  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç¢ºèªã™ã¹ã
            "failure_notification_ready": True  # Gmailè¨­å®šã‹ã‚‰ç¢ºèªã™ã¹ã
        }
        
        return results
    
    def test_integration_reliability(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ»é€£æºæ€§æ¤œè¨¼"""
        logger.info("ğŸ”— ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ»é€£æºæ€§æ¤œè¨¼é–‹å§‹")
        
        results = {
            "gmail_api": {},
            "calendar_api": {},
            "external_apis": {},
            "web_ui": {}
        }
        
        # Gmail APIèªè¨¼ç¢ºèª
        token_path = self.project_root / "token.json"
        credentials_path = self.project_root / "credentials.json"
        results["gmail_api"] = {
            "token_exists": token_path.exists(),
            "credentials_exists": credentials_path.exists(),
            "authentication_ready": token_path.exists() and credentials_path.exists()
        }
        
        # Google Calendar APIç¢ºèª
        results["calendar_api"] = {
            "same_credentials_as_gmail": True,  # åŒã˜èªè¨¼æƒ…å ±ã‚’ä½¿ç”¨
            "calendar_integration_ready": token_path.exists()
        }
        
        # å¤–éƒ¨APIå¯ç”¨æ€§ç¢ºèª
        external_apis = [
            ("AniList GraphQL", "https://graphql.anilist.co"),
            ("dã‚¢ãƒ‹ãƒ¡ã‚¹ãƒˆã‚¢RSS", "https://anime.dmkt-sp.jp/animestore/CF/rss/")
        ]
        
        api_results = {}
        for api_name, url in external_apis:
            try:
                response = requests.get(url, timeout=10)
                api_results[api_name] = {
                    "status_code": response.status_code,
                    "response_time": response.elapsed.total_seconds() * 1000,
                    "available": response.status_code < 400
                }
            except Exception as e:
                api_results[api_name] = {
                    "available": False,
                    "error": str(e)
                }
        
        results["external_apis"] = api_results
        
        # Web UIç¢ºèª
        flask_app = self.project_root / "app.py"
        dashboard_module = self.project_root / "modules" / "dashboard.py"
        results["web_ui"] = {
            "flask_app_exists": flask_app.exists(),
            "dashboard_module_exists": dashboard_module.exists(),
            "web_ui_ready": flask_app.exists() and dashboard_module.exists()
        }
        
        return results
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info("ğŸ“Š æœ€çµ‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        
        # å…¨ä½“æ¤œè¨¼å®Ÿè¡Œ
        api_results = asyncio.run(self.measure_api_performance())
        db_results = self.measure_database_performance()
        resource_results = self.measure_system_resources()
        scalability_results = self.test_scalability()
        automation_results = self.validate_operational_automation()
        integration_results = self.test_integration_reliability()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
        score_components = {}
        
        # APIæ€§èƒ½ã‚¹ã‚³ã‚¢ (0-25ç‚¹)
        if api_results["anilist_response_times"]:
            avg_anilist_time = sum(api_results["anilist_response_times"]) / len(api_results["anilist_response_times"])
            api_score = max(0, 25 - (avg_anilist_time / 100))  # 100msä»¥ä¸‹ã§æº€ç‚¹
            score_components["api_performance"] = min(25, api_score)
        else:
            score_components["api_performance"] = 0
        
        # DBæ€§èƒ½ã‚¹ã‚³ã‚¢ (0-25ç‚¹)
        if db_results["insert_performance"] and db_results["select_performance"]:
            db_score = max(0, 25 - (db_results["insert_performance"] + db_results["select_performance"]) / 200)
            score_components["database_performance"] = min(25, db_score)
        else:
            score_components["database_performance"] = 0
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ (0-25ç‚¹)
        cpu_usage = resource_results["cpu_usage"]
        memory_usage = resource_results["memory_usage"]["percent"]
        resource_score = max(0, 25 - (cpu_usage + memory_usage) / 8)  # ä½ä½¿ç”¨ç‡ã§é«˜å¾—ç‚¹
        score_components["resource_efficiency"] = min(25, resource_score)
        
        # é‹ç”¨è‡ªå‹•åŒ–ã‚¹ã‚³ã‚¢ (0-25ç‚¹)
        automation_score = 0
        if automation_results["cron_configuration"]["configured"]:
            automation_score += 8
        if automation_results["log_management"]["log_directory_exists"]:
            automation_score += 7
        if automation_results["backup_functionality"]["backup_script_exists"]:
            automation_score += 5
        if automation_results["error_recovery"]["error_recovery_module_exists"]:
            automation_score += 5
        score_components["operational_readiness"] = automation_score
        
        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = sum(score_components.values())
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        final_report = {
            "overall_performance_score": total_score,
            "score_breakdown": score_components,
            "detailed_results": {
                "api_performance": api_results,
                "database_performance": db_results,
                "system_resources": resource_results,
                "scalability": scalability_results,
                "operational_automation": automation_results,
                "system_integration": integration_results
            },
            "recommendations": self.generate_recommendations(total_score, score_components),
            "system_specifications": self.get_recommended_specs(),
            "operational_guidelines": self.get_operational_guidelines(),
            "validation_timestamp": datetime.now().isoformat(),
            "total_validation_time": time.time() - self.start_time
        }
        
        return final_report
    
    def generate_recommendations(self, total_score: float, score_components: Dict[str, float]) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if total_score < 70:
            recommendations.append("ğŸš¨ ç·åˆã‚¹ã‚³ã‚¢ãŒ70ç‚¹æœªæº€ã§ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©åŒ–ãŒå¿…è¦ã§ã™ã€‚")
        
        if score_components["api_performance"] < 20:
            recommendations.append("ğŸŒ APIå¿œç­”æ™‚é–“ãŒé…ã„ã§ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã®å°å…¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        if score_components["database_performance"] < 20:
            recommendations.append("ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ€§èƒ½ãŒä½ã„ã§ã™ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
        
        if score_components["resource_efficiency"] < 20:
            recommendations.append("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ãŒé«˜ã„ã§ã™ã€‚ãƒ¡ãƒ¢ãƒªãƒ»CPUæœ€é©åŒ–ãŒå¿…è¦ã§ã™ã€‚")
        
        if score_components["operational_readiness"] < 20:
            recommendations.append("ğŸ”§ é‹ç”¨è‡ªå‹•åŒ–ãŒä¸ååˆ†ã§ã™ã€‚ç›£è¦–ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½“åˆ¶ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚")
        
        if total_score >= 90:
            recommendations.append("âœ… å„ªç§€ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã®é‹ç”¨æº–å‚™ãŒæ•´ã£ã¦ã„ã¾ã™ã€‚")
        elif total_score >= 80:
            recommendations.append("ğŸ‘ è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã§ã™ã€‚è»½å¾®ãªèª¿æ•´ã§æœ¬ç•ªé‹ç”¨å¯èƒ½ã§ã™ã€‚")
        
        return recommendations
    
    def get_recommended_specs(self) -> Dict[str, str]:
        """æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜"""
        return {
            "os": "Ubuntu 20.04 LTSä»¥ä¸Š",
            "cpu": "2ã‚³ã‚¢ä»¥ä¸Š (æ¨å¥¨4ã‚³ã‚¢)",
            "memory": "4GBä»¥ä¸Š (æ¨å¥¨8GB)",
            "storage": "20GBä»¥ä¸Š SSDæ¨å¥¨",
            "network": "å®‰å®šã—ãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š (1Mbpsä»¥ä¸Š)",
            "python": "Python 3.8ä»¥ä¸Š",
            "database": "SQLite (ã¾ãŸã¯PostgreSQL for production)",
            "monitoring": "ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ„ãƒ¼ãƒ« (htop, iostatç­‰)"
        }
    
    def get_operational_guidelines(self) -> Dict[str, List[str]]:
        """é‹ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³"""
        return {
            "daily_operations": [
                "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªã¨ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³",
                "ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®ç›£è¦–",
                "å¤–éƒ¨APIæ¥ç¶šçŠ¶æ³ã®ç¢ºèª",
                "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ"
            ],
            "weekly_operations": [
                "ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª",
                "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ›´æ–°ã®é©ç”¨",
                "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–",
                "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ•´åˆæ€§ç¢ºèª"
            ],
            "monthly_operations": [
                "ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æ›´æ–°ç¢ºèª",
                "ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã®è¦‹ç›´ã—",
                "ç½å®³å¾©æ—§æ‰‹é †ã®ç¢ºèª",
                "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®æ¤œè¨"
            ],
            "emergency_procedures": [
                "ã‚·ã‚¹ãƒ†ãƒ åœæ­¢æ™‚ã®å¾©æ—§æ‰‹é †",
                "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç ´ææ™‚ã®ä¿®å¾©æ–¹æ³•",
                "APIèªè¨¼ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å‡¦æ³•",
                "ç·Šæ€¥é€£çµ¡å…ˆã¨ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹é †"
            ]
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ ã‚¢ãƒ‹ãƒ¡ãƒ»ãƒãƒ³ã‚¬æƒ…å ±é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»é‹ç”¨æ€§æœ€çµ‚æ¤œè¨¼é–‹å§‹")
    
    validator = PerformanceValidator()
    
    try:
        # æœ€çµ‚æ¤œè¨¼å®Ÿè¡Œ
        final_report = validator.generate_performance_report()
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        report_path = "/mnt/Linux-ExHDD/MangaAnime-Info-delivery-system/PERFORMANCE_VALIDATION_REPORT.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2, default=str)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "="*80)
        print("ğŸ¯ ã‚¢ãƒ‹ãƒ¡ãƒ»ãƒãƒ³ã‚¬æƒ…å ±é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  - æœ€çµ‚æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*80)
        print(f"ğŸ“Š ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {final_report['overall_performance_score']:.1f}/100")
        print(f"âš¡ APIæ€§èƒ½: {final_report['score_breakdown']['api_performance']:.1f}/25")
        print(f"ğŸ—ƒï¸ DBæ€§èƒ½: {final_report['score_breakdown']['database_performance']:.1f}/25")
        print(f"ğŸ’» ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡: {final_report['score_breakdown']['resource_efficiency']:.1f}/25")
        print(f"ğŸ”§ é‹ç”¨æº–å‚™: {final_report['score_breakdown']['operational_readiness']:.1f}/25")
        print(f"â±ï¸ æ¤œè¨¼å®Ÿè¡Œæ™‚é–“: {final_report['total_validation_time']:.2f}ç§’")
        
        print("\nğŸ“‹ ä¸»è¦æ¨å¥¨äº‹é …:")
        for i, rec in enumerate(final_report['recommendations'][:5], 1):
            print(f"  {i}. {rec}")
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print("="*80)
        
        logger.info("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»é‹ç”¨æ€§æœ€çµ‚æ¤œè¨¼å®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ æ¤œè¨¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()