#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
db.sqlite3ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    python3 scripts/analyze_database.py
"""

import sqlite3
import os
import json
from datetime import datetime
from pathlib import Path


class DatabaseAnalyzer:
    # è¨±å¯ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    ALLOWED_TABLES = {'works', 'releases', 'notification_history', 'settings',
                      'rss_feeds', 'api_sources', 'calendar_events'}

    def __init__(self, db_path):
        self.db_path = db_path
        self.conn = None
        self.report = {
            'analyzed_at': datetime.now().isoformat(),
            'database_path': str(db_path),
            'database_size_mb': 0,
            'tables': {},
            'indexes': [],
            'pragma_settings': {},
            'data_quality': {},
            'recommendations': []
        }

    def _validate_identifier(self, identifier: str) -> bool:
        """è­˜åˆ¥å­ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚«ãƒ©ãƒ åï¼‰ã®å®‰å…¨æ€§ã‚’æ¤œè¨¼"""
        import re
        # è‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿è¨±å¯
        return bool(re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', identifier))

    def _safe_identifier(self, identifier: str) -> str:
        """å®‰å…¨ãªè­˜åˆ¥å­ã‚’è¿”ã™ï¼ˆæ¤œè¨¼å¤±æ•—æ™‚ã¯ä¾‹å¤–ï¼‰"""
        if not self._validate_identifier(identifier):
            raise ValueError(f"ä¸æ­£ãªè­˜åˆ¥å­: {identifier}")
        return identifier

    def connect(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š"""
        if not os.path.exists(self.db_path):
            logger.info(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.db_path}")
            return False

        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row
            return True
        except Exception as e:
            logger.info(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_database_info(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã‚’å–å¾—"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
        self.report['database_size_mb'] = round(
            os.path.getsize(self.db_path) / (1024 * 1024), 2
        )

        # PRAGMAè¨­å®š
        pragma_queries = [
            'journal_mode',
            'synchronous',
            'cache_size',
            'temp_store',
            'foreign_keys',
            'auto_vacuum'
        ]

        for pragma in pragma_queries:
            cursor = self.conn.execute(f'PRAGMA {pragma}')
            value = cursor.fetchone()[0]
            self.report['pragma_settings'][pragma] = value

    def get_table_list(self):
        """å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        cursor = self.conn.execute("""
            SELECT name FROM sqlite_master
            WHERE type = 'table' AND name NOT LIKE 'sqlite_%'
            ORDER BY name
        """)
        return [row['name'] for row in cursor.fetchall()]

    def get_table_schema(self, table_name):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—"""
        cursor = self.conn.execute(f"PRAGMA table_info({table_name})")
        columns = []
        for row in cursor.fetchall():
            columns.append({
                'cid': row['cid'],
                'name': row['name'],
                'type': row['type'],
                'notnull': bool(row['notnull']),
                'default_value': row['dflt_value'],
                'pk': bool(row['pk'])
            })

        # CREATE TABLEæ–‡ã‚’å–å¾—
        cursor = self.conn.execute("""
            SELECT sql FROM sqlite_master
            WHERE type = 'table' AND name = ?
        """, (table_name,))
        create_sql = cursor.fetchone()['sql']

        return {
            'columns': columns,
            'create_sql': create_sql
        }

    def get_table_statistics(self, table_name):
        """ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¤œè¨¼ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
        safe_table = self._safe_identifier(table_name)
        stats = {}

        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼ˆè­˜åˆ¥å­ã¯æ¤œè¨¼æ¸ˆã¿ãªã®ã§å®‰å…¨ï¼‰
        cursor = self.conn.execute(f"SELECT COUNT(*) FROM [{safe_table}]")
        stats['row_count'] = cursor.fetchone()[0]

        # å„ã‚«ãƒ©ãƒ ã®NULLæ•°ã¨ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°
        schema = self.get_table_schema(table_name)
        column_stats = {}

        for col in schema['columns']:
            col_name = col['name']
            # ã‚«ãƒ©ãƒ åã‚‚æ¤œè¨¼
            safe_col = self._safe_identifier(col_name)
            col_stats = {}

            # NULLæ•°
            cursor = self.conn.execute(
                f"SELECT COUNT(*) FROM [{safe_table}] WHERE [{safe_col}] IS NULL"
            )
            col_stats['null_count'] = cursor.fetchone()[0]

            # ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°
            try:
                cursor = self.conn.execute(
                    f"SELECT COUNT(DISTINCT [{safe_col}]) FROM [{safe_table}]"
                )
                col_stats['distinct_count'] = cursor.fetchone()[0]
            except Exception:
                col_stats['distinct_count'] = None

            column_stats[col_name] = col_stats

        stats['columns'] = column_stats
        return stats

    def get_indexes(self):
        """å…¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—"""
        cursor = self.conn.execute("""
            SELECT name, tbl_name, sql
            FROM sqlite_master
            WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
            ORDER BY tbl_name, name
        """)

        indexes = []
        for row in cursor.fetchall():
            indexes.append({
                'name': row['name'],
                'table': row['tbl_name'],
                'sql': row['sql']
            })

        return indexes

    def check_data_quality(self):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯"""
        quality_issues = []

        tables = self.get_table_list()

        # worksãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        if 'works' in tables:
            # å¿…é ˆã‚«ãƒ©ãƒ ã®NULLãƒã‚§ãƒƒã‚¯
            cursor = self.conn.execute("SELECT COUNT(*) FROM works WHERE title IS NULL")
            null_titles = cursor.fetchone()[0]
            if null_titles > 0:
                quality_issues.append({
                    'severity': 'high',
                    'table': 'works',
                    'issue': f'{null_titles}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã§titleãŒNULL'
                })

            # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ãƒã‚§ãƒƒã‚¯
            cursor = self.conn.execute("""
                SELECT title, COUNT(*) as count
                FROM works
                GROUP BY title
                HAVING count > 1
            """)
            duplicates = cursor.fetchall()
            if duplicates:
                quality_issues.append({
                    'severity': 'medium',
                    'table': 'works',
                    'issue': f'{len(duplicates)}ä»¶ã®é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«',
                    'examples': [{'title': row[0], 'count': row[1]} for row in duplicates[:5]]
                })

        # releasesãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        if 'releases' in tables:
            # å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆwork_idãŒå­˜åœ¨ã—ãªã„ï¼‰
            if 'works' in tables:
                cursor = self.conn.execute("""
                    SELECT COUNT(*) FROM releases r
                    LEFT JOIN works w ON r.work_id = w.id
                    WHERE w.id IS NULL
                """)
                orphaned = cursor.fetchone()[0]
                if orphaned > 0:
                    quality_issues.append({
                        'severity': 'critical',
                        'table': 'releases',
                        'issue': f'{orphaned}ä»¶ã®å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆå¯¾å¿œã™ã‚‹worksãŒå­˜åœ¨ã—ãªã„ï¼‰'
                    })

            # æœªé€šçŸ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ç¢ºèª
            cursor = self.conn.execute("""
                SELECT COUNT(*) FROM releases
                WHERE notified = 0 AND release_date <= DATE('now')
            """)
            pending_notifications = cursor.fetchone()[0]
            if pending_notifications > 0:
                quality_issues.append({
                    'severity': 'info',
                    'table': 'releases',
                    'issue': f'{pending_notifications}ä»¶ã®æœªé€šçŸ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆé…ä¿¡æ—¥ã‚’éãã¦ã„ã‚‹ï¼‰'
                })

        return quality_issues

    def get_recommendations(self):
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒã‚§ãƒƒã‚¯
        indexes = self.get_indexes()
        index_names = [idx['name'] for idx in indexes]

        required_indexes = {
            'idx_works_title': 'works',
            'idx_works_type': 'works',
            'idx_releases_work_id': 'releases',
            'idx_releases_date': 'releases',
            'idx_releases_notified': 'releases'
        }

        for idx_name, table in required_indexes.items():
            if idx_name not in index_names:
                recommendations.append({
                    'priority': 'high',
                    'category': 'index',
                    'suggestion': f'{table}ãƒ†ãƒ¼ãƒ–ãƒ«ã«{idx_name}ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„',
                    'sql': self._get_index_sql(idx_name)
                })

        # PRAGMAè¨­å®šãƒã‚§ãƒƒã‚¯
        if self.report['pragma_settings'].get('journal_mode') != 'wal':
            recommendations.append({
                'priority': 'high',
                'category': 'configuration',
                'suggestion': 'WALãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¦ãã ã•ã„',
                'sql': 'PRAGMA journal_mode = WAL;'
            })

        if self.report['pragma_settings'].get('foreign_keys') != 1:
            recommendations.append({
                'priority': 'high',
                'category': 'configuration',
                'suggestion': 'å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚’æœ‰åŠ¹åŒ–ã—ã¦ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ä¿è¨¼ã—ã¦ãã ã•ã„',
                'sql': 'PRAGMA foreign_keys = ON;'
            })

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if self.report['database_size_mb'] > 100:
            recommendations.append({
                'priority': 'medium',
                'category': 'maintenance',
                'suggestion': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºãŒ100MBã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚VACUUMã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„',
                'sql': 'VACUUM;'
            })

        return recommendations

    def _get_index_sql(self, index_name):
        """æ¨å¥¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®SQLæ–‡ã‚’è¿”ã™"""
        index_sqls = {
            'idx_works_title': 'CREATE INDEX idx_works_title ON works(title);',
            'idx_works_type': 'CREATE INDEX idx_works_type ON works(type);',
            'idx_releases_work_id': 'CREATE INDEX idx_releases_work_id ON releases(work_id);',
            'idx_releases_date': 'CREATE INDEX idx_releases_date ON releases(release_date);',
            'idx_releases_notified': 'CREATE INDEX idx_releases_notified ON releases(notified, release_date);'
        }
        return index_sqls.get(index_name, '')

    def analyze(self):
        """å®Œå…¨åˆ†æã‚’å®Ÿè¡Œ"""
        if not self.connect():
            return None

        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        logger.info(f"å¯¾è±¡: {self.db_path}")
        logger.info()

        # åŸºæœ¬æƒ…å ±
        logger.info("1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã‚’å–å¾—ä¸­...")
        self.get_database_info()

        # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
        logger.info("2. ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—ä¸­...")
        tables = self.get_table_list()
        for table in tables:
            logger.info(f"   - {table}")
            schema = self.get_table_schema(table)
            stats = self.get_table_statistics(table)
            self.report['tables'][table] = {
                'schema': schema,
                'statistics': stats
            }

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±
        logger.info("3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’å–å¾—ä¸­...")
        self.report['indexes'] = self.get_indexes()

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        logger.info("4. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ä¸­...")
        self.report['data_quality']['issues'] = self.check_data_quality()

        # æ¨å¥¨äº‹é …
        logger.info("5. æœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆä¸­...")
        self.report['recommendations'] = self.get_recommendations()

        logger.info()
        logger.info("åˆ†æå®Œäº†!")
        return self.report

    def save_report(self, output_path):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, ensure_ascii=False, indent=2)
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    def print_summary(self):
        """ã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›"""
import logging

logger = logging.getLogger(__name__)

        logger.info("\n" + "=" * 80)

logger = logging.getLogger(__name__)

        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æã‚µãƒãƒªãƒ¼")
        logger.info("=" * 80)

        logger.info(f"\nãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {self.report['database_path']}")
        logger.info(f"ã‚µã‚¤ã‚º: {self.report['database_size_mb']} MB")
        logger.info(f"åˆ†ææ—¥æ™‚: {self.report['analyzed_at']}")

        logger.info("\n--- ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ ---")
        for table_name, table_data in self.report['tables'].items():
            stats = table_data['statistics']
            logger.info(f"{table_name}: {stats['row_count']:,} ãƒ¬ã‚³ãƒ¼ãƒ‰")

        logger.info("\n--- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ ---")
        if self.report['indexes']:
            for idx in self.report['indexes']:
                logger.info(f"  {idx['name']} on {idx['table']}")
        else:
            logger.info("  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆè¦ä½œæˆï¼ï¼‰")

        logger.info("\n--- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯çµæœ ---")
        issues = self.report['data_quality']['issues']
        if issues:
            for issue in issues:
                severity_icon = {
                    'critical': 'ğŸ”´',
                    'high': 'ğŸŸ ',
                    'medium': 'ğŸŸ¡',
                    'info': 'ğŸ”µ'
                }.get(issue['severity'], 'âšª')
                logger.info(f"  {severity_icon} [{issue['severity'].upper()}] {issue['issue']}")
        else:
            logger.info("  å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        logger.info("\n--- æœ€é©åŒ–æ¨å¥¨äº‹é … ---")
        if self.report['recommendations']:
            for rec in self.report['recommendations']:
                priority_icon = {
                    'high': 'ğŸ”´',
                    'medium': 'ğŸŸ¡',
                    'low': 'ğŸŸ¢'
                }.get(rec['priority'], 'âšª')
                logger.info(f"  {priority_icon} [{rec['priority'].upper()}] {rec['suggestion']}")
        else:
            logger.info("  æ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“")

        logger.info("\n" + "=" * 80)

    def close(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹"""
        if self.conn:
            self.conn.close()


def main():
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
    project_root = Path(__file__).parent.parent
    db_path = project_root / 'db.sqlite3'

    analyzer = DatabaseAnalyzer(db_path)
    report = analyzer.analyze()

    if report:
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
        report_path = project_root / 'docs' / 'technical' / 'database-analysis-report.json'
        analyzer.save_report(report_path)

        # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        analyzer.print_summary()

        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã¸ã®ãƒ‘ã‚¹è¡¨ç¤º
        logger.info(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        logger.info(f"æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰: {project_root / 'docs' / 'technical' / 'database-optimization-report.md'}")

    analyzer.close()


if __name__ == '__main__':
    main()
